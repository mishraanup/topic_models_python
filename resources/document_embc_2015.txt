
Abstract— In this paper, we present an interactive physical therapy system (IPTS) for remote quantitative assessment of clients in the home. The system consists of two different interactive interfaces connected through a network, for a real-time low latency video conference using audio, video, skeletal, and depth data streams from a Microsoft Kinect. To test the potential of IPTS, experiments were conducted with 5 independent living senior subjects in Kansas City, MO. Also, experiments were conducted in the lab to validate the real-time biomechanical measures calculated using the skeletal data from the Microsoft Xbox 360 Kinect and Microsoft Xbox One Kinect, with ground truth data from a Vicon motion capture system. Good agreements were found in the validation tests. The results show potential capabilities of the IPTS system to provide remote physical therapy to clients, especially older adults, who may find it difficult to visit the clinic.
I. INTRODUCTION
Physical therapy (PT) is a major rehabilitation methodology to improve postural control and functional abilities. A study in 2009 showed only 6.2 physical therapists per 10,000 people across the United States . Many PT clients, especially older adults, may have a hard time getting to the clinic due to distance or logistics (e.g., rural residents and those in large cities). Given the challenges faced in providing physical therapy services due to limited facilities and human resources, a new class of smart tele-health systems is proposed to deliver quality and faster services. A tele-health system can help with early interventions, better health outcome with fewer clinical visits, and faster one-on-one interactive care. In this paper, we introduce an interactive remote PT system that helps patients to “see” the therapist more frequently so they can be assessed more frequently, get their exercises updated if necessary, and make sure they are performing them correctly with real-time feedback from the therapist. Also, not all clinics have the expensive commercial PT equipment and not everyone has access to them because of scheduling issues. So with the use of this proposed application, more people will have easier access to quantitative measurement of their gait and balance.
There have been studies on different biomechanical features, such as Trunk Sway and motion of the 5th Lumbar vertebra (L5), which provide important information about human postural balance conditions . However, capturing these features is a challenging task, as it involves expensive three dimensional (3D) camera motion capture systems, force plate systems, or accelerometer systems. An inexpensive and highly portable alternative that can evaluate a PT client through capturing these biomechanical features has a great potential to augment the conventional systems. The Microsoft Kinect sensors with the software development kit are capable of capturing the 3D joint locations of the human body. The Kinect RGB camera and microphone array also facilitates its use as a video conferencing tool.
There have been a number of studies investigating the accuracy of the skeletal model provided by the Kinect. In , Livingston et al. evaluated the performance of the Kinect skeletal model in various aspects and explored the possibilities of using it as a gesture recognition device. In , Ross et al. explained the usefulness of the Kinect to create a real-time biofeedback system for gait retraining. In another study , Bonnechere et al. determined the effectiveness of the Kinect in functional assessment tests as compared to a marker based system. The Kinect performed better in upper body measurements compared to lower. Also, in  and , the performance of the Microsoft Xbox 360 Kinect (K1) is compared with the recently introduced Microsoft Xbox One Kinect (K2). In , Xu and McGorry compared the static joint locations with a conventional opto-electric motion tracking system but did not compute any dynamic biomechanical measure. In , Malinowski and Matsinos examined the similarity and dissimilarity in some biomechanical measures between the motion data from the two Kinects but did not compare them with any gold standard motion tracking system. Again, in , Pfister et al. used only K1 to measure some biomechanical measures but did not compute trunk sway which is necessary in our study.
There is also prior work in using the Kinect for remote PT . In , two different VR applications were designed using the Kinect and Vicon systems, for both local and remote PT. Avatars of the PT client and the therapist are created in real-time within the applications; communications between the client and therapist is conducted through the Avatar models. In another study , a motion capture application is proposed for remote orthopedics rehabilitation. This system uses a set of gaming motion capture technologies (e.g., Wii, Kinect, and PlayStation Move) for exercise data collection in different scenarios involving clinic and home sites. In their remote sessions, the exercise data are not exchanged in real-time through a videoconferencing setup, but are sent over a network for offline visualization and feedback. The inability of both these systems to establish a live, real-time video feedback limits the operation and usefulness of the system.
Here, we present an interactive video conferencing application using Kinect sensors that establishes a live audio-video communication between the therapist and PT client, with detailed real-time analysis of joint alignments and trunk sway from the skeletal data of the client in the home. A post-activity result window provides a trace of L5 motion in the horizontal plane, maximum trunk sway values in anteroposterior (AP) and mediolateral (ML) directions for single leg stance (SLS) and tandem walk (TW), with eyes open and eyes closed tests. A Vicon motion capture system was used for ground truth comparison. Also, the K1 was compared with the K2 to verify the performance of both with respect to the Vicon system.
Section II of this paper includes a brief description of the two Kinect sensors, the architecture of the interactive PT system, followed by a brief explanation of the interface components and the biomechanical measures calculated in real-time for the two PT activities. Section III contains the results of a study conducted to validate results from the K1 and K2 systems using ground truth data from the Vicon on each measure. Finally, section IV contains a brief discussion of the results, including the ongoing work.
II. Methodology
A.  Microsoft Kinect Sensors
The K1 and K2 Kinect sensors, shown in Fig. 1 (a) and (b), are two widely used inexpensive depth sensors for motion analysis. Kinect K1 uses an infrared sensitive camera that generates a depth image from a pattern of actively emitted infrared light. Kinect K2 uses a time-of-flight phase detection camera system to get the depth image . They both generate depth images independent of ambient lighting. The Kinect Software Development Kit 1.8 for K1 and 2.0 for K2 provided by Microsoft help in fitting 20 point and 25 point skeletal models, respectively, to segmented human bodies . Skeletal models generated by both systems are shown in Fig. 1 (c) and (d). In this study, we investigate the K1 and K2 Kinects and evaluate the accuracy and stability of trunk sway measured by each system.
B.  Interactive Physical Therapy System (IPTS)
The IPTS consists of two interactive interfaces, one for the PT client in the home and another for the therapist in the clinic. The application interfaces were developed with iterative feedback provided by a therapist with an active clinical practice. Fig. 2 (a) shows the therapist’s interface window, which provides real-time visual feedback of kinematic information, measured from the Kinect skeletal data. This includes the AP and ML trunk sway, full body joint alignments, a real-time view of the client’s skeletal model, and the status of the network. Dropdown menus and buttons are included to select the subject ID, PT activity, and trial number for each PT activity. Every selection provides a visual feedback by providing information about the selected activity and trial at the bottom of the interface window, and by changing color of the selected trial. Each activity trial is timed for 20 seconds. Finally, a result window displays the post-activity assessment for all the activities and trials, including a top-down view of the horizontal L5 motion and the maximum AP and ML trunk-sway. Fig. 3 (a) and (b) show the horizontal L5 traces generated for different trials of SLS and TW tests. The result window also displays the actual task performance time in the case of SLS tests, out of the 20 second trial period. Moreover, the therapist’s interface supports a set of voice commands to interact with the application, improving user experience.
The interface window for the PT client in the home, displayed in Fig. 2 (b) has a relatively simple interface to help her focus on the activity without distraction. The client interface window includes a timer bar that indicates the time left to perform a particular PT activity and live trunk sway feedback that helps the client to maintain a better form as she performs the activities. These unique features of the IPTS combined with a basic video and audio communication makes it more than just a video conference system.
A modified version of  is used to establish a peer to peer communication between the therapist in the clinic and the PT client in the home using RGB, depth, skeletal, and audio data streams received from the Kinect SDK. The local and remote Kinect data are processed and displayed on the interfaces as per the user requirements and available features.  For each PT activity the Kinect depth data and the post-activity results are saved in a local database.
These data can be visualized using the same PT interfaces. A public database was used to save IP address data to establish the peer to peer communication. Fig. 4 shows the block diagram of the IPTS.
    Figure 1.  (a): K1; (b): K2; (c): skeletal model generated by the Microsoft Kinect SDK 1.8 on depth images captured by K1; (d): skeletal model generated by Microsoft Kinect SDK 2.0 on depth images captured by K2

    Figure 2.  Application interface windows for the (a): Therapist; (b): In-home client.

    Figure 3.  Horizontal L5 traces for the (a): SLS test; (b) TW test.
    Figure 4.  Block diagram of IPTS.
C.  Biomechanical Measures
The 3D joint location data provided by the Kinect were used to calculate the joint alignments and trunk sway measures. Fig. 5 illustrates the AP and ML trunk sway measures, which are estimated by calculating the angle made by the shoulder center joint with respect to the vertical Y axis at the hip center joint location in AP and ML directions, respectively. The trunk sway measures are calculated as:
        
        
Where  and are the AP and ML trunk sway, and (XHic, YHiC, ZHiC) and (XShC, YShC, ZShC) are the 3D joint locations for hip center and shoulder center, respectively. The four joint alignments measured were head to neck, shoulder center to spine, hip to knee and knee to foot (left and right). For any given frame, the joint alignment was calculated as the angle made by one joint with respect to the vertical axis of the other. The joint alignment can be expressed as,
        
Where  represents the joint alignment, (X1, Y1, Z1) and (X2, Y2, Z2) are the 3D coordinates of the two joints. For each SLS  tests,  the actual performance time was measured   using

    Figure 5.  Trunk Sway measures
the joint alignment measures of hip to knee () and knee to ankle (). Any given depth frame is considered as an actual SLS performed frame, if it satisfies any of the two conditions provided in (4) and (5).
       
        
Where and are the minimum threshold values of and respectively.
In this study, we considered = 7 degrees and = 10 degrees, based on best results. Finally, we have assumed the spinal joint as the L5 of the human body for the horizontal traces of body sway in the post-activity results.
III. Results
A Vicon motion capture system was used to validate the biomechanical features measured using the skeletal data from the Kinect SDKs as described in section II. The two Kinect SDKs produce different sets of 3D skeletal joint positions. The 3D skeletal position data from the sternum (SDK 1.8: shoulder center and SDK 2.0: spine shoulder) and hip center (SDK 1.8: hip center and SDK 2.0: spine base) were used to calculate the trunk sway. The data collection for Kinect K1 was done using the Windows 7 operating system (OS), whereas the Windows 8.1 OS was used to collect the data for Kinect K2. Windows 7 was used for K1 due to substantial frame drops in Windows 8.1. For the Vicon system, markers were attached to the subjects’ shoulders, back, and hip and their 3D positions were recorded in synchronization with the Kinects. Four subjects participated in this validation test. The subjects were asked to perform 3 trials for each of the tests: single leg stance with eyes open (SLS EO), single leg stance with eyes closed (SLS EC), tandem walk with eyes open (TW EO), and tandem walk with eyes closed (TW EC). The tandem walk task was defined as a five step heel to toe walk. Tables I and II show the comparisons between the each Kinect type and the Vicon system. In every test, K2 performed better in calculating AP trunk sway as compared to K1. In SLS tests, the maximum average AP trunk sway error  measured by K1 was 2.0 2.5 whereas it was -0.30.8 by K2. The maximum average ML trunk sway error measured in SLS tests were 3.12.2 by K1 and 2.41.5 by K2.
    TABLE I.  Trunk Sway Angle Error for the Single Leg Stance Test Compared to the Vicon
    TABLE II.  Trunk Sway angle Error for the Tandem Walk Test Compared to the Vicon

In TW tests, the maximum average trunk sway error  measured in the AP direction by K1 and K2 were 3.44.6 and -0.61.1 respectively. However, the maximum average trunk sway error  measured in the ML direction were 1.51.4 by K1 and 1.71.0 by K2. Results show that the performance of K1 in measuring ML trunk sway was slightly better than K2 in most cases but the results were very close.  For performance across all of the trials, the maximum trunk sway error measured by K1 in the AP and ML directions were 17.24 degree and 7.29 degree, respectively. The maximum trunk sway error measures in the AP and ML directions for K2 were 12.75 degree and 5.63 degree, respectively. Overall, K2 had a more stable skeletal model and larger range for activity tracking than K1.
In order to evaluate the IPTS, 5 older adults from Kansas City were recruited to take part in an IRB-approved human subjects study. Four tests were done using the IPTS, with a physical therapist in Columbia, MO and each subject in the home in Kansas City, MO. A survey form was used to obtain feedback about the interface and the overall system after each test, which helped to improve the system further. At the final test after development was complete, the therapist rated the technical quality of audio and video and the user satisfaction and PT interaction as high as 4.7 and 4.8 out of 5 respectively, based on 17 individual ratings.
IV. Conclusion
We have developed a novel Kinect-based interactive PT system for remote assessment of PT clients, especially targeting older adult users. Several tests were done with five independent living seniors to assess the usability, user satisfaction, and overall efficiency of the IPTS over the network. The biomechanical features measured by the system were validated with a Vicon motion capture system, especially the trunk sway in the AP and ML directions with very good agreement being achieved. The results of this study indicate that the Kinect skeletal models constructed by the K1 and K2 systems offer acceptable accuracy for use as a part of a remote PT system. Moreover, the high user satisfaction ratings provided by the therapist demonstrate the potential of the system for a broader impact in tele-health.
Our future work will consider feedback from a larger group of PT clients and therapists to make the user experience better. The system will be validated with gold standard PT assessment systems, such as force-plate and accelerometer systems. One of the limitations of this study was the sample size of individuals considered for Kinect validation. A larger sample size will be considered for the future validations.
Acknowledgment
The authors would like to acknowledge the members in Center for Eldercare and Rehabilitation Technology.
References

[1] M. D. Landry, Thomas C Ricketts, E. Fraher, and M. C. Verrier, "Physical Therapy Health Human Resource Ratios: A Comparative Analysis of the United States and Canada," Physical Therapy, pp. 149-161, 2009.
[2] Z. Halická, J. Lobotková, K. Bučková, and F. Hlavačka, "Effectiveness of different visual biofeedback signals for human balance improvement," Gait & Posture, vol. 39, pp. 410–414, 2013.
[3] A. L. Adkin, B. R. Bloem, and J. H. J. Allum, "Trunk sway measurements during stance and gait tasks in Parkinson's disease," Gait & Posture, vol. 22, pp. 240–249, 2004.
[4] MA Livingston, J Sebastian, Z Ai, and J. Decker, "Performance Measurements of the Microsoft Kinect Skeleton," in Virtual Reality Short Papers and Posters (VRW), 2012.
[5] Ross A. Clarka, Yong-Hao Puac, Adam L. Bryantb, and M. A. Huntd, "Validity of the Microsoft Kinect for providing lateral trunk lean feedback during gait retraining," Gait & Posture, vol. 38, pp. 1064–1066, 2013.
[6] B. Bonnechere, B. Jansen, P. Salvia, H. Bouzahouene, L. Omelina, F. Moiseev , et al., "Validity and reliability of the Kinect within functional assessment activities: Comparison with standard stereophotogrammetry," Gait & Posture, vol. 39, pp. 593–598, 2014.
[7] X. Xu and R. W. McGorry, "The validity of the first and second generation Microsoft Kinect™ for identifying joint center locations during static postures," Applied Ergonomics, vol. 49, pp. 47-54, 2015.
[8] M. Malinowski and E. Matsinos, "Comparative study of the two versions of the Microsoft Kinect sensor in regard to the analysis of human motion," arXiv preprint arXiv:1504.00221, 2015.
[9] A. Pfister, A. M. West, S. Bronner, and J. A. Noah, "Comparative abilities of Microsoft Kinect and Vicon 3D motion capture for gait analysis," Journal of medical engineering & technology, vol. 38, pp. 274-280, 2014.
[10]    C. Camporesi, M. Kallmann, and J. J. Han, "VR solutions for improving physical therapy," in Virtual Reality (VR), 2013 IEEE, 2013, pp. 77-78.
[11]    D. Tacconi, R. Tomasi, C. Costa, and O. Mayora, "A system for remote orthopedics rehabilitation," in Proc., Intl. Conf. on Pervasive Computing Technologies for Healthcare, 2013, pp. 313-314.
[12]    J. Sell and P. O'Connor, "The Xbox One System on a Chip and Kinect Sensor," Micro, IEEE, vol. 32, pp. 44-53, 2014.
[13]    Microsoft, "Kinect for Windows SDK 2.0 Documentation," 2014.
[14]    Microsoft, "Kinect for Windows SDK 1.8 Documentation," 2012.
[15]    Crutkas, Danielfe, and Peekb, "Coding4Fun Kinect Service v1.6 Documentation," 2012.

